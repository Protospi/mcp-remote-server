{
  "2502.09100v1": {
    "title": "Logical Reasoning in Large Language Models: A Survey",
    "authors": [
      "Hanmeng Liu",
      "Zhizhang Fu",
      "Mengru Ding",
      "Ruoxi Ning",
      "Chaoli Zhang",
      "Xiaozhang Liu",
      "Yue Zhang"
    ],
    "summary": "With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2502.09100v1",
    "published": "2025-02-13"
  },
  "2410.12509v1": {
    "title": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions",
    "authors": [
      "Ilias Tachmazidis",
      "Sotiris Batsakis",
      "Grigoris Antoniou"
    ],
    "summary": "Large Language Models (LLMs) have gained prominence in the AI landscape due\nto their exceptional performance. Thus, it is essential to gain a better\nunderstanding of their capabilities and limitations, among others in terms of\nnonmonotonic reasoning. This paper proposes a benchmark that corresponds to\nvarious defeasible rule-based reasoning patterns. We modified an existing\nbenchmark for defeasible logic reasoners by translating defeasible rules into\ntext suitable for LLMs. We conducted preliminary experiments on nonmonotonic\nrule-based reasoning using ChatGPT and compared it with reasoning patterns\ndefined by defeasible logic.",
    "pdf_url": "http://arxiv.org/pdf/2410.12509v1",
    "published": "2024-10-16"
  },
  "2501.16961v2": {
    "title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
    "authors": [
      "Mohammad Raza",
      "Natasa Milic-Frayling"
    ],
    "summary": "Robustness of reasoning remains a significant challenge for large language\nmodels, and addressing it is essential for the practical applicability of\nAI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a\nnovel approach that addresses the key challenge in combining language models\nwith the rigor of logical solvers: to accurately formulate the reasoning\nproblem from natural language to the formal language of the solver. SSV uses a\nconsistency-based approach to produce strong abstract formalizations of\nproblems using concrete instantiations that are generated by the model and\nverified by the solver. In addition to significantly advancing the overall\nreasoning accuracy over the state-of-the-art, a key novelty that this approach\npresents is a feature of verification that has near-perfect precision over a\nsignificant coverage of cases, as we demonstrate on open reasoning benchmarks.\nWe propose such *near-certain reasoning* as a new approach to reduce the need\nfor manual verification in many cases, taking us closer to more dependable and\nautonomous AI reasoning systems.",
    "pdf_url": "http://arxiv.org/pdf/2501.16961v2",
    "published": "2025-01-28"
  },
  "2408.16081v1": {
    "title": "Logic-Enhanced Language Model Agents for Trustworthy Social Simulations",
    "authors": [
      "Agnieszka Mensfelt",
      "Kostas Stathis",
      "Vince Trencsenyi"
    ],
    "summary": "We introduce the Logic-Enhanced Language Model Agents (LELMA) framework, a\nnovel approach to enhance the trustworthiness of social simulations that\nutilize large language models (LLMs). While LLMs have gained attention as\nagents for simulating human behaviour, their applicability in this role is\nlimited by issues such as inherent hallucinations and logical inconsistencies.\nLELMA addresses these challenges by integrating LLMs with symbolic AI, enabling\nlogical verification of the reasoning generated by LLMs. This verification\nprocess provides corrective feedback, refining the reasoning output. The\nframework consists of three main components: an LLM-Reasoner for producing\nstrategic reasoning, an LLM-Translator for mapping natural language reasoning\nto logic queries, and a Solver for evaluating these queries. This study focuses\non decision-making in game-theoretic scenarios as a model of human interaction.\nExperiments involving the Hawk-Dove game, Prisoner's Dilemma, and Stag Hunt\nhighlight the limitations of state-of-the-art LLMs, GPT-4 Omni and Gemini 1.0\nPro, in producing correct reasoning in these contexts. LELMA demonstrates high\naccuracy in error detection and improves the reasoning correctness of LLMs via\nself-refinement, particularly in GPT-4 Omni.",
    "pdf_url": "http://arxiv.org/pdf/2408.16081v1",
    "published": "2024-08-28"
  },
  "2502.15361v1": {
    "title": "Evaluating Social Biases in LLM Reasoning",
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "summary": "In the recent development of AI reasoning, large language models (LLMs) are\ntrained to automatically generate chain-of-thought reasoning steps, which have\ndemonstrated compelling performance on math and coding tasks. However, when\nbias is mixed within the reasoning process to form strong logical arguments, it\ncould cause even more harmful results and further induce hallucinations. In\nthis paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against\ntheir instruction tuned counterparts on the BBQ dataset, and investigated the\nbias that is elicited out and being amplified through reasoning steps. To the\nbest of our knowledge, this empirical study is the first to assess bias issues\nin LLM reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2502.15361v1",
    "published": "2025-02-21"
  }
}